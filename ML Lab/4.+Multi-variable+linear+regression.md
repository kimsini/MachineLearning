
# LAB 04 : multi-variable linear regression

이번에는 다양한 variable(feature) 을 이용한 회귀를 공부한다.
전에 공부했던 걸 요약하자면 아래와 같이 요약할 수 있다.
1. Hypothesis : H(x) = W*x + b
2. Cost function : cost(W,b) = (1/m) * (H(xi) - yi)^2
3. Gradient descent Algorithm for minimizing cost

코세라에서도 이미 공부했던 거라 더 자세히 알고 있다.
이 유튜브 강의는 딥러닝 강의라 회귀 파트는 간단히 하는 것 같다.

![pic](https://3.bp.blogspot.com/-ZMbMJD2sgy0/V49LIb_IGxI/AAAAAAAAHg0/pZg6wlICfooIiTuPe17pwhIZpNFkWh-bQCLcB/s400/%25EC%25BA%25A1%25EC%25B2%2598.PNG)

위 표처럼 variable 이 2가지가 존재한다.
공부하 시간과 학습 태도를 가지고 점수를 예측하는 것이다.
위 내용을 식으로 표현하면 아래와 같이 표현할 수 있다.

![pic](https://2.bp.blogspot.com/-Bah4OvnJhXw/V49Lx7o2RqI/AAAAAAAAHg4/yHQAjgBc0YUTRcHSsQWE9-vvzU8teLxlQCLcB/s400/%25EC%25BA%25A1%25EC%25B2%2598.PNG)

위 식을 매트릭스로 표현하는 것이 좋다.

![pic](https://1.bp.blogspot.com/-lc_SznE8VJk/V49MGlHzvPI/AAAAAAAAHhA/pAU9-nmIRBQSVtop9K4kKWon93Dd8LmWgCLcB/s400/%25EC%25BA%25A1%25EC%25B2%2598.PNG)

트랜스포스를 이용해 행렬에서 곱셈을 이용할 수 있다.

위 과정을 텐서플로우로 옮겨보자.

```python
import tensorflow as tf
import numpy as np

#파일 첫줄에 "#"이 있는 것은 주석으로 처리하여 읽지 않음
xy = np.loadtxt('data.txt',unpack=True,dtype='float32')
#첫줄부터 맨마지막 전까지 (-1은 끝에서 부터 세는것)
x_data = xy[0:-1]
#y데이터는 맨 마지막 번째
y_data = xy[-1]

# x_data = [[1.,0.,3.,0.,5.],[0.,2.,0.,4.,0.]]
# y_data = [1,2,3,4,5]

print x_data
print y_data

#4개의 데이터
W = tf.Variable(tf.random_uniform([1,len(x_data)],-5.0,5.0))

hyp = tf.matmul(W,x_data)

cost = tf.reduce_mean(tf.square(hyp-y_data))

optimizer = tf.train.GradientDescentOptimizer(1e-2)
train = optimizer.minimize(cost)

session = tf.Session()
session.run(tf.initialize_all_variables())

for step in range(5000):
    session.run(train)
    if(step%100 ==0):
        print step, session.run(cost), session.run(W)
```
