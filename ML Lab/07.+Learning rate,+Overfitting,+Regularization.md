# lec 07-1: Learning rate, Overfitting, Regularization
-------------------------------------------------------
우선 Learning Rate에 대해 간단히 알아보면, gradient descent에서 한번씩 움직일 때, 한번 움직이는 정도를 learning rate라고한다.

![pic](https://2.bp.blogspot.com/-MkNwEUT1aDE/V7CfHGXXx_I/AAAAAAAAH-U/WQlesDGTVpUXgPX7JrRgv5q7qqCRiBiUQCK4B/s400/ScreenShot_20160812235353.png)

움직이는 정도가 너무 커버리면 최소화를 넘어버리는 경우가 발생한다.
그러나 너무 작으면 시간이 오래 걸린다. 그러므로 적당한 learning rate를 찾아 실행시켜야한다. 

![pic](https://1.bp.blogspot.com/-3X3v9DScb9E/V7CfhV3xMEI/AAAAAAAAH-c/eE8WpcKt8AcoGfbw2jtBbojFp0bC7DVDACK4B/s320/ScreenShot_20160812235353.png)

이번에는 learning rate도 적당한 크기로 했는데도 cost함수가 발산한다면,
데이터의 크기 차이를 봐야한다.
위 그림처럼 데이터의 차이가 크다면 수렴이 안될수도 있다.
그러므로 전처리를 해주어야한다.

![pic](https://1.bp.blogspot.com/-irYw_i82t3U/V7Cf9TSJxsI/AAAAAAAAH-o/fnvvchYGZA46k0IufcvcMqZyArPCKGGFQCK4B/s400/ScreenShot_20160812235353.png)

위 그림처럼 어느 범위에 데이터가 모이도록하는 normalize시켜야한다.
기본적으로 standardization이 있다.

이번에는 overfitting을 해결하는 방법에 대해 알아보자.

![pic](https://2.bp.blogspot.com/-4-iTNdE0ILA/V7CgUdKmNlI/AAAAAAAAH-w/wUlBz_gA2hw_0QoJ9cJBa8-zHmwQsxgLwCK4B/s400/ScreenShot_20160812235353.png)

위 3가지가 있다. 3번째인 일반화시키는 것을 자세히 알아보자면
regularization의 기본 원리는 weight를 줄이는 것이다.

![pic](https://4.bp.blogspot.com/-0QwMiglTLcA/V7Cglv-rPWI/AAAAAAAAH-4/X3qNBoUUipUSsQdvN_qEJwEZ6II0c2h_wCK4B/s400/ScreenShot_20160812235353.png)

overfitting은 그래프가 많이 구부려진것이므로 weight를 줄여 그래프를 구부리지 말고 피자는 것이다. 식으로 나타내보면 아래와 같다.

![pic](https://4.bp.blogspot.com/-4lfe0r03qIk/V7Cg7aE8OYI/AAAAAAAAH_A/ylYe7BA_sacNruO4tNd1cfZoBwyGSnlOgCK4B/s400/ScreenShot_20160812235353.png)

위처럼 식의 마지막 부분에 weight를 처리하면서 람다를 이용해 regularization의 중요도를 조절할 수 있다.

# lec 07-2: Training/Testing data set
-------------------------------------------
우리가 만든 모델을 어떻게 평가할까.
기본적으로 training set 으로 공부하고 test set 으로 시험을 보는 것이다.

![pic](https://2.bp.blogspot.com/-VtSKa4I6ADg/V7GGoJ7W2gI/AAAAAAAAH_Q/QkKR4xHJ7gA0la6nQ_DWFj7wCzgZO0jLwCK4B/s400/ScreenShot_20160812235353.png)

데이터의 양이 많으면 validation set을 가지고 알파나 람다를 튜닝하는 과정도 필요하다.
그리고 test set을 이용해 시험을 보는 것이다.
validation set은 모의 고사 후 오답 공부하는 느낌이다.

또 다른 방법으로 online learning 이라는 방법이 있다.

![pic](https://4.bp.blogspot.com/-47fkhEh4mmY/V7GHb25pClI/AAAAAAAAH_Y/82sEVyEIa6UyMLh5JLBNX7nNrTg1w0-RACK4B/s400/ScreenShot_20160812235353.png)

데이터의 양이 streaming으로 계속 들어오는 유형이라면, 하루에 100만개의 데이터가 들어올 때 10만개씩 데이터를 추가하면서 weight를 튜닝한다는 느낌으로 학습한다. 즉, 10만개가 들어올 때마다 모델링을 새로하는 것이 아니라 weights를 고치는 것이다.

# Reference
----------------------------------------------------------
* https://www.youtube.com/user/hunkims/playlists
