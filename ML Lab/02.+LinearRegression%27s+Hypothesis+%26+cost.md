
이번에는 선형 회귀에 대해 공부한다. 강의하시는 교수님께서
1. Andrew Ng's ML class
2. CS231n
3. Tensorflow
위 3가지 강의 자료를 편집해서 강의하십니다.

# Lab 02 : Linear Regression의 Hypothesis와 Cost

선형 회귀의 간단한 개념은 데이터를 학습시켜 모델링하여 만들어진 인공지능에 자신이 원하는 값을 넣으면 어떤 예측을 하는지 알 수 있는 것이라고 합니다.

선형 회귀에서 우리의 가설은 H(x) = Wx + b 라고 나타냅니다. 그리고 가설과 데이터의 차이를 나타내는 함수는 cost function 이라고 합니다. cost function 을 나타내보면, ( H(x) - y ) ^ 2 으로 나타낼 수 있습니다. 마이너스 값도 존재할 수 있기 때문에 제곱을 하여 cost 를 나타냅니다. 우리의 목표는 cost 를 가장 작게 하는 것입니다.

![pic](https://3.bp.blogspot.com/-HKXV8eejKfg/V44CNRxwGaI/AAAAAAAAHgA/0k2jaI-_wmE02eEUWkXQOK5_zhCXNtlbwCLcB/s400/%25EC%25BA%25A1%25EC%25B2%2598.PNG)

이제 tensorflow에서 어떻게 돌아가는지 알아보겠습니다.

```python
import tensorflow as tf

#값 설정
x_data = [1,2,3]
y_data = [1,2,3]

# -1.0 부터 1.0까지의 수중 랜덤하게 갖는 변수를 정의함
W = tf.Variable(tf.random_uniform([1],-1.0,1.0))
b = tf.Variable(tf.random_uniform([1],-1.0,1.0))

#1차 방정식 ax+b의 식을 세움
hyp = W * x_data + b

# 손실함수는 (가정 - 실제)^2 의 평균으로 연산을 정의함
cost = tf.reduce_mean(tf.square( hyp - y_data ))

#손실함수를 최소로 만들도록 정의함
optimizer = tf.train.GradientDescentOptimizer(1e-2)
train = optimizer.minimize(cost)

#W,B의 변수들을 초기화 시켜주는 연산을 정의함
init = tf.initialize_all_variables()

session = tf.Session()
session.run(init)

for step in range(2001):
    session.run(train)
    if(step%20 == 0):
        print step,session.run(cost),session.run(W),session.run(b)
```

저번 게시글에서 변수에 값을 넣는다고 연산이 일어나는 것이 아니라는 것을 배웠습니다.
session.run 이 일어나야 연산이 일어납니다. 그리고 이번에는 모든 변수들을 초기화시키는tf.initialize_all_variables() 을 session.run 전에 실행하여 초기화시켜야합니다. 

train 을 연산하는 과정이 아직 이해가지 않습니다. 텐서플로우 라이브러리에 적응하지 못했나봅니다.

# Reference
----------------------------------------------------------
* https://www.youtube.com/user/hunkims/playlists
